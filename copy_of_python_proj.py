# -*- coding: utf-8 -*-
"""Copy of python_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4su-z92cuO2lAn_0Tathu2WoPO9g77H
"""

import csv

file_path = "lung_cancer.csv"

# Load the CSV data
with open(file_path, "r") as file:
    csv_reader = csv.DictReader(file)
    raw_data = [row for row in csv_reader]

# Display the first 5 rows
print("Loaded Data Sample:", raw_data[:5])



import csv

# File path to the CSV
file_path = "lung_cancer.csv"

# Load the CSV data
with open(file_path, "r") as file:
    csv_reader = csv.DictReader(file)
    raw_data = [row for row in csv_reader]

# Display the first 5 rows to verify loading
print("Loaded Data Sample:", raw_data[:5])

# Process the data into the Patients Table
patients = []
patient_ids = set()

for i, row in enumerate(raw_data):
    if i not in patient_ids:
        patients.append({
            "patient_id": i,
            "age": row["AGE"],  # Replace with the actual column name for age in your dataset
            "sex": row["GENDER"]   # Replace with the actual column name for sex in your dataset
        })
        patient_ids.add(i)

# Display the first 5 rows of the Patients Table
print("Patients Table:", patients[:5])

medical_records = []
record_ids = set()

for i, row in enumerate(raw_data):
    if i not in record_ids:
        medical_records.append({
            "record_id": i,
            "patient_id": i,  # Use the same patient_id as in the Patients table
            "smoking": row["SMOKING"],
            "coughing": row["COUGHING"],
            "shortness_of_breath": row["SHORTNESS_OF_BREATH"],
            "chest_pain": row["CHEST_PAIN"]
        })
        record_ids.add(i)

# Display the first 5 rows of the Medical Records Table
print("Medical Records Table:", medical_records[:5])

diagnosis = []
diagnosis_ids = set()

for i, row in enumerate(raw_data):
    if i not in diagnosis_ids:
        diagnosis.append({
            "diagnosis_id": i,
            "patient_id": i,  # Use the same patient_id as in the Patients table
            "lung_cancer": row["LUNG_CANCER"]
        })
        diagnosis_ids.add(i)

# Display the first 5 rows of the Diagnosis Table
print("Diagnosis Table:", diagnosis[:5])

import csv

# Function to save data to CSV
def save_to_csv(file_name, data, headers):
    with open(file_name, "w", newline="") as file:
        writer = csv.DictWriter(file, fieldnames=headers)
        writer.writeheader()
        writer.writerows(data)

# Save Patients Table
save_to_csv("patients.csv", patients, ["patient_id", "age", "sex"])

# Save Medical Records Table
save_to_csv("medical_records.csv", medical_records, ["record_id", "patient_id", "smoking", "coughing", "shortness_of_breath", "chest_pain"])

# Save Diagnosis Table
save_to_csv("diagnosis.csv", diagnosis, ["diagnosis_id", "patient_id", "lung_cancer"])

print("Normalized tables saved successfully!")

# Function to read and display CSV content
def read_csv(file_name):
    with open(file_name, "r") as file:
        reader = csv.DictReader(file)
        return [row for row in reader]

# Reload and display data
patients_data = read_csv("patients.csv")
medical_records_data = read_csv("medical_records.csv")
diagnosis_data = read_csv("diagnosis.csv")

print("Patients Data Sample:", patients_data[:5])
print("Medical Records Data Sample:", medical_records_data[:5])
print("Diagnosis Data Sample:", diagnosis_data[:5])

import sqlite3

# Create SQLite connection
connection = sqlite3.connect("lung_cancer.db")
cursor = connection.cursor()

# Create tables in SQLite
cursor.execute("""
CREATE TABLE IF NOT EXISTS Patients (
    patient_id INTEGER PRIMARY KEY,
    age INTEGER,
    sex TEXT
);
""")

cursor.execute("""
CREATE TABLE IF NOT EXISTS MedicalRecords (
    record_id INTEGER PRIMARY KEY,
    patient_id INTEGER,
    smoking TEXT,
    coughing TEXT,
    shortness_of_breath TEXT,
    chest_pain TEXT,
    FOREIGN KEY (patient_id) REFERENCES Patients(patient_id)
);
""")

cursor.execute("""
CREATE TABLE IF NOT EXISTS Diagnosis (
    diagnosis_id INTEGER PRIMARY KEY,
    patient_id INTEGER,
    lung_cancer TEXT,
    FOREIGN KEY (patient_id) REFERENCES Patients(patient_id)
);
""")
connection.commit()

print("Database tables created successfully!")

# Insert data into Patients table
for patient in patients_data:
    cursor.execute("""
    INSERT INTO Patients (patient_id, age, sex)
    VALUES (?, ?, ?);
    """, (patient["patient_id"], patient["age"], patient["sex"]))

# Insert data into MedicalRecords table
for record in medical_records_data:
    cursor.execute("""
    INSERT INTO MedicalRecords (record_id, patient_id, smoking, coughing, shortness_of_breath, chest_pain)
    VALUES (?, ?, ?, ?, ?, ?);
    """, (record["record_id"], record["patient_id"], record["smoking"], record["coughing"], record["shortness_of_breath"], record["chest_pain"]))

# Insert data into Diagnosis table
for diag in diagnosis_data:
    cursor.execute("""
    INSERT INTO Diagnosis (diagnosis_id, patient_id, lung_cancer)
    VALUES (?, ?, ?);
    """, (diag["diagnosis_id"], diag["patient_id"], diag["lung_cancer"]))

connection.commit()

print("Data inserted into SQLite database successfully!")

# Perform SQL JOIN to fetch combined data
query = """
SELECT Patients.patient_id, Patients.age, Patients.sex,
       MedicalRecords.smoking, MedicalRecords.coughing,
       MedicalRecords.shortness_of_breath, MedicalRecords.chest_pain,
       Diagnosis.lung_cancer
FROM Patients
JOIN MedicalRecords ON Patients.patient_id = MedicalRecords.patient_id
JOIN Diagnosis ON Patients.patient_id = Diagnosis.patient_id;
"""

cursor.execute(query)
combined_data = cursor.fetchall()

# Display combined data
print("Combined Data Sample:", combined_data[:5])

pip install pandas

import pandas as pd
print("Pandas version:", pd.__version__)

import sqlite3
import pandas as pd

# Connect to the SQLite database
connection = sqlite3.connect("lung_cancer.db")

# SQL JOIN query to combine normalized tables
query = """
SELECT Patients.patient_id, Patients.age, Patients.sex,
       MedicalRecords.smoking, MedicalRecords.coughing,
       MedicalRecords.shortness_of_breath, MedicalRecords.chest_pain,
       Diagnosis.lung_cancer
FROM Patients
JOIN MedicalRecords ON Patients.patient_id = MedicalRecords.patient_id
JOIN Diagnosis ON Patients.patient_id = Diagnosis.patient_id;
"""

# Execute the query and load the data into a Pandas DataFrame
df = pd.read_sql_query(query, connection)

# Display the first 5 rows of the DataFrame
print(df.head())

# Optionally save the DataFrame to a CSV file for later use
df.to_csv("combined_data.csv", index=False)

# Close the database connection
connection.close()

# Check distribution of the target variable
print("Lung Cancer Distribution:\n", df["lung_cancer"].value_counts(normalize=True))

!pip install scikit-learn

import sklearn
print("scikit-learn version:", sklearn.__version__)

from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop(columns=["lung_cancer"])  # Features
y = df["lung_cancer"]                # Target

# Perform train/test split with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Verify class distribution in training and test sets
print("Training Set Lung Cancer Distribution:\n", y_train.value_counts(normalize=True))
print("Test Set Lung Cancer Distribution:\n", y_test.value_counts(normalize=True))

# General information about the dataset
print("Dataset Info:")
print(df.info())

# Summary statistics
print("\nSummary Statistics:")
print(df.describe(include="all"))

!pip install seaborn

# Convert 'sex' to numeric (0 for 'M', 1 for 'F')
df['sex'] = df['sex'].map({'M': 0, 'F': 1})

# Ensure all other categorical columns (e.g., 'smoking') are numeric
df['smoking'] = pd.to_numeric(df['smoking'], errors='coerce')
df['coughing'] = pd.to_numeric(df['coughing'], errors='coerce')
df['shortness_of_breath'] = pd.to_numeric(df['shortness_of_breath'], errors='coerce')
df['chest_pain'] = pd.to_numeric(df['chest_pain'], errors='coerce')

# Convert 'lung_cancer' to numeric (0 for 'NO', 1 for 'YES')
df['lung_cancer'] = df['lung_cancer'].map({'NO': 0, 'YES': 1})

# Convert other string columns (e.g., 'sex') to numeric if not already done
if 'sex' in df.columns:
    df['sex'] = df['sex'].map({'M': 0, 'F': 1})

print(df.dtypes)

# Compute the correlation matrix
correlation_matrix = df.corr()

# Display the correlation matrix as a heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix")
plt.show()


# The correlation matrix shows that all features have very low correlation with the target variable lung_cancer, indicating no strong
# linear relationships. Features like smoking, coughing, shortness_of_breath, and chest_pain are largely independent of each other.
# The dataset includes both categorical features (sex, smoking, etc.) and numerical features (age), where sex requires encoding (M/F to 0/1) and
# age needs to be checked for outliers or capped values. Missing values should be handled by imputing numerical features (e.g., age) with
# the mean/median and categorical features with the mode. The column patient_id is irrelevant for modeling and should be dropped.To prepare
# the data, binary features need to be ensured as numeric, and scaling (e.g., StandardScaler or MinMaxScaler) should be applied to age.
# Additionally, the class balance for lung_cancer should be checked, and if imbalanced, resampling techniques like SMOTE can be applied.

pip install scikit-learn mlflow

from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop(columns=["lung_cancer"])  # Drop the target column
y = df["lung_cancer"]                # Target column

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression

# Define preprocessing for numerical features
numeric_features = ["age"]  # Add more numeric features as necessary
numeric_transformer = Pipeline(steps=[
    ("scaler", StandardScaler())  # Use MinMaxScaler if needed
])

# Define preprocessing for categorical features
categorical_features = ["sex", "smoking", "coughing", "shortness_of_breath", "chest_pain"]
categorical_transformer = Pipeline(steps=[
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Define the complete pipeline
pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression())
])

from sklearn.model_selection import GridSearchCV

# Define hyperparameter grid
param_grid = {
    "classifier__C": [0.01, 0.1, 1, 10],  # Regularization strength
    "classifier__penalty": ["l2"]         # Regularization type
}

# Perform grid search with 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring="f1", n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best parameters and CV results
print("Best Parameters:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)

from sklearn.metrics import f1_score, confusion_matrix

# Predict on test data
y_pred = grid_search.predict(X_test)

# Calculate metrics
f1 = f1_score(y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print("F1 Score:", f1)
print(f"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}")

pip install dagshub

import mlflow
import mlflow.sklearn
import dagshub

# Set up DagsHub MLFlow remote tracking URI

import dagshub
dagshub.init(repo_owner='manaswini.chittepu4', repo_name='MANASWINI_PROJECT', mlflow=True)
mlflow.set_tracking_uri("https://dagshub.com/manaswini.chittepu4/MANASWINI_PROJECT/experiments")  # Replace with your DagsHub URL

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, confusion_matrix, make_scorer

# Separate features and target
X = df.drop(columns=["lung_cancer"])  # Features
y = df["lung_cancer"]                # Target

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Preprocessing Steps
numeric_features = ["age"]  # Replace with other numeric columns
categorical_features = ["sex", "smoking", "coughing", "shortness_of_breath", "chest_pain"]

# Define preprocessing transformers
preprocessor = ColumnTransformer(
    transformers=[
        ("num_standard", StandardScaler(), numeric_features),
        ("num_minmax", MinMaxScaler(), numeric_features),
        ("num_log", FunctionTransformer(np.log1p, validate=True), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
    ]
)

# Combine preprocessing and Logistic Regression in a pipeline
pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(solver="liblinear", max_iter=1000))
])

# Define hyperparameter grid
param_grid = {
    "classifier__C": [0.01, 0.1, 1, 10],  # Regularization strength
    "classifier__penalty": ["l1", "l2"]   # Regularization type
}

# Define F1-Score as Scorer
f1_scorer = make_scorer(f1_score)

# Cross-Validation with GridSearchCV
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring=f1_scorer, n_jobs=-1)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Mean CV F1 Score:", grid_search.best_score_)

import mlflow
import dagshub



# Initialize DagsHub and MLFlow integration


dagshub.init(repo_owner='manaswini.chittepu4', repo_name='MANASWINI_PROJECT', mlflow=True)

mlflow.set_tracking_uri("https://dagshub.com/manaswini.chittepu4/MANASWINI_PROJECT.mlflow")

# Check for missing values
print("Missing values in training data:\n", X_train.isnull().sum())
print("Missing values in test data:\n", X_test.isnull().sum())

# Example: Fill missing values with the mean for numerical columns
X_train = X_train.fillna(X_train.mean())
X_test = X_test.fillna(X_test.mean())

# Example input after conversion and missing value handling
input_example = X_train.iloc[:1]

with mlflow.start_run():
    mlflow.log_params(grid_search.best_params_)
    mlflow.log_metric("f1_score", f1)

    # Log model with corrected schema
    mlflow.sklearn.log_model(
        sk_model=grid_search.best_estimator_,
        artifact_path="logistic_regression_pipeline",
        input_example=input_example
    )
    print("Model logged successfully with corrected schema!")

# Example: Replace `df` with your DataFrame
X = df.drop(columns=["lung_cancer"])  # Features
y = df["lung_cancer"]                # Target

# Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing steps
numeric_features = ["age"]  # Replace with your numeric features
categorical_features = ["sex", "smoking", "coughing", "shortness_of_breath", "chest_pain"]

preprocessor = ColumnTransformer(transformers=[
    ("num_standard", StandardScaler(), numeric_features),
    ("num_minmax", MinMaxScaler(), numeric_features),
    ("num_log", FunctionTransformer(np.log1p), numeric_features),
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
])

# Combine preprocessing and Logistic Regression
pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("classifier", LogisticRegression(solver="liblinear", max_iter=1000))
])

# Define hyperparameters for GridSearchCV
param_grid = {
    "classifier__C": [0.01, 0.1, 1, 10],
    "classifier__penalty": ["l1", "l2"]
}

# Cross-validation
f1_scorer = make_scorer(f1_score)

grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring=f1_scorer, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Best hyperparameters and CV results
print("Best Parameters:", grid_search.best_params_)
print("Mean CV F1 Score:", grid_search.best_score_)

# Predictions
y_pred = grid_search.predict(X_test)

# F1-score and confusion matrix
f1 = f1_score(y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print("F1 Score:", f1)
print(f"Confusion Matrix: TP={tp}, TN={tn}, FP={fp}, FN={fn}")

input_example = X_train.iloc[:1]  # Example input for logging

with mlflow.start_run():
    # Log hyperparameters
    mlflow.log_params(grid_search.best_params_)

    # Log metrics
    mlflow.log_metric("f1_score", f1)
    mlflow.log_metric("true_positives", tp)
    mlflow.log_metric("true_negatives", tn)
    mlflow.log_metric("false_positives", fp)
    mlflow.log_metric("false_negatives", fn)
    mlflow.log_metric("mean_cv_f1_score", grid_search.best_score_)

    # Log the best model
    mlflow.sklearn.log_model(
        sk_model=grid_search.best_estimator_,
        artifact_path="logistic_regression_pipeline",
        input_example=input_example
    )

    print("Experiment logged to MLFlow and DagsHub successfully!")

pip install scikit-learn mlflow dagshub xgboost

import mlflow
import dagshub



# Initialize DagsHub and MLFlow integration


dagshub.init(repo_owner='manaswini.chittepu4', repo_name='MANASWINI_PROJECT', mlflow=True)

mlflow.set_tracking_uri("https://dagshub.com/manaswini.chittepu4/MANASWINI_PROJECT.mlflow")

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import numpy as np
import pandas as pd

pip install --upgrade scikit-learn

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import mlflow
import mlflow.sklearn

# Load Dataset
df = pd.read_csv('lung_cancer.csv')

# Define Features and Target
X = df.drop(columns=["LUNG_CANCER"])  # Replace "LUNG_CANCER" with the correct target column name
y = df["LUNG_CANCER"]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess: Encode non-numeric features and scale the data
X_train_encoded = pd.get_dummies(X_train, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, drop_first=True)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

# Custom Feature Engineering: Interaction Features
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

# Get new feature names
feature_names = poly.get_feature_names_out(input_features=X_train_encoded.columns)

# Start MLFlow Experiment
mlflow.set_experiment("Custom_Experiment_InteractionFeatures")

with mlflow.start_run(run_name="Interaction_Features_LogisticRegression"):
    # Train a Logistic Regression Model
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_poly, y_train)
    y_pred = model.predict(X_test_poly)

    # Evaluate Model
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="weighted")

    # Log Results in MLFlow
    mlflow.log_param("method", "Interaction Features with Logistic Regression")
    mlflow.log_param("degree", 2)
    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("f1_score", f1)
    mlflow.log_text("\n".join(feature_names), "interaction_features.txt")
    mlflow.sklearn.log_model(model, "InteractionFeatures_LogisticRegression_Model")

    # Print Results
    print("Model Accuracy:", accuracy)
    print("Model F1-Score:", f1)
    print("Custom Interaction Features Experiment Complete. Results logged in MLFlow.")

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import mlflow
import mlflow.sklearn

# Load Dataset
df = pd.read_csv('lung_cancer.csv')

# Define Features and Target
X = df.drop(columns=["LUNG_CANCER"])  # Replace "LUNG_CANCER" with the correct column name if needed
y = df["LUNG_CANCER"]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess: Encode non-numeric features and scale the data
X_train_encoded = pd.get_dummies(X_train, drop_first=True)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)

# Start MLFlow Experiment
mlflow.set_experiment("PCA_Dimensionality_Reduction")

# PCA for Dimensionality Reduction
with mlflow.start_run(run_name="PCA_Scree_Plot"):
    # Perform PCA
    pca = PCA()
    X_train_pca = pca.fit_transform(X_train_scaled)

    # Scree Plot (Explained Variance)
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)

    # Create the Scree Plot
    plt.figure(figsize=(8, 6))
    plt.plot(range(1, len(explained_variance) + 1), cumulative_variance, marker='o', linestyle='--')
    plt.xlabel('Number of Principal Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Scree Plot for PCA')
    plt.grid()
    plt.savefig("scree_plot.png")  # Save the plot locally
    plt.show()

    # Log results in MLFlow
    mlflow.log_param("method", "PCA")
    mlflow.log_metric("n_components_95_variance", np.argmax(cumulative_variance >= 0.95) + 1)
    mlflow.log_artifact("scree_plot.png")  # Log scree plot image

    print("Number of components to retain 95% variance:", np.argmax(cumulative_variance >= 0.95) + 1)

# Convert all integer columns to float64
for col in X.columns:
    if X[col].dtype == 'int64':
        X[col] = X[col].astype('float64')

# Ensure there are no missing values
X = X.fillna(0)  # Replace NaN with 0, or choose a more appropriate strategy

# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
import mlflow
import dagshub

# Set MLflow with DagsHub
dagshub.init(repo_owner='manaswini.chittepu4', repo_name='MANASWINI_PROJECT', mlflow=True)
mlflow.set_tracking_uri("https://dagshub.com/manaswini.chittepu4/MANASWINI_PROJECT.mlflow")

# Load dataset
data = pd.read_csv("lung_cancer.csv")
data.columns = data.columns.str.strip().str.upper()

# Convert target column 'LUNG_CANCER' to binary format
data['LUNG_CANCER'] = data['LUNG_CANCER'].map({'NO': 0, 'YES': 1})

# Step 1: Feature Engineering - Combine and Create New Features
data['AGE_SMOKER'] = data['AGE'] * data['SMOKING']
data['LIFESTYLE_SCORE'] = data['FATIGUE'] + data['ALCOHOL_CONSUMING']
data['WHEEZING_CHESTPAIN'] = np.where((data['WHEEZING'] == 1) & (data['CHEST_PAIN'] == 1), 1, 0)

# Separate features and target
X = data.drop(columns=['LUNG_CANCER'])
y = data['LUNG_CANCER']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 2: Preprocessing Pipeline
numeric_features = ['AGE', 'AGE_SMOKER', 'LIFESTYLE_SCORE']
categorical_features = ['GENDER', 'SMOKING', 'ANXIETY', 'WHEEZING_CHESTPAIN']

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric_features),
    ('cat', OneHotEncoder(), categorical_features)
])

# Step 3: Models
models = {
    'LogisticRegression': LogisticRegression(),
    'RidgeClassifier': RidgeClassifier(),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'XGBClassifier': XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

# Step 4: Train, Evaluate, and Log Results in MLflow
mlflow.autolog()  # Enable autologging for all models

for model_name, model in models.items():
    with mlflow.start_run(run_name=model_name):
        # Create pipeline
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', model)
        ])

        # Train model
        pipeline.fit(X_train, y_train)

        # Cross-validation
        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')
        mean_cv = np.mean(cv_scores)
        std_cv = np.std(cv_scores)

        # Log metrics and parameters
        mlflow.log_param('model_name', model_name)
        mlflow.log_metric('mean_cv_accuracy', mean_cv)
        mlflow.log_metric('std_cv_accuracy', std_cv)


        print(f"{model_name} - CV Accuracy: {mean_cv:.4f} ± {std_cv:.4f}, Test Accuracy: {test_accuracy:.4f}")

#6

mlflow.set_experiment("Custom_Experiment_RFE_RF")

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
import mlflow
import mlflow.sklearn

# Load Dataset
df = pd.read_csv('lung_cancer.csv')

# Define Features and Target
X = df.drop(columns=["LUNG_CANCER"])  # Replace "LUNG_CANCER" with the correct target column name
y = df["LUNG_CANCER"]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess: Encode non-numeric features and scale the data
X_train_encoded = pd.get_dummies(X_train, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, drop_first=True)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

# Custom Feature Engineering: Interaction Features
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

# Get new feature names
feature_names = poly.get_feature_names_out(input_features=X_train_encoded.columns)

# Start MLFlow Experiment
mlflow.set_experiment("Custom_Experiment_InteractionFeatures")

with mlflow.start_run(run_name="Interaction_Features_LogisticRegression"):
    # Train a Logistic Regression Model
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_poly, y_train)
    y_pred = model.predict(X_test_poly)

    # Evaluate Model
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average="weighted")

    # Log Results in MLFlow
    mlflow.log_param("method", "Interaction Features with Logistic Regression")
    mlflow.log_param("degree", 2)
    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("f1_score", f1)
    mlflow.log_text("\n".join(feature_names), "interaction_features.txt")
    mlflow.sklearn.log_model(model, "InteractionFeatures_LogisticRegression_Model")

    # Print Results
    print("Model Accuracy:", accuracy)
    print("Model F1-Score:", f1)
    print("Custom Interaction Features Experiment Complete. Results logged in MLFlow.")

#7

mlflow.set_experiment("Custom_Experiment_TimeBasedFeatures")

# Convert y_train to NumPy array to avoid Series issues
y_train = y_train.values

import os
os.environ["MLFLOW_DISABLE_DATSET_AUTOLOGGING"] = "TRUE"

import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.preprocessing import StandardScaler
import mlflow
import mlflow.sklearn

# Disable automatic dataset logging
os.environ["MLFLOW_DISABLE_DATSET_AUTOLOGGING"] = "TRUE"

# Load Dataset
df = pd.read_csv('lung_cancer.csv')

# Define Features and Target
X = df.drop(columns=["LUNG_CANCER"])  # Replace "LUNG_CANCER" with the correct target column name
y = df["LUNG_CANCER"]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocess: Encode non-numeric features and scale the data
X_train_encoded = pd.get_dummies(X_train, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, drop_first=True)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_encoded)
X_test_scaled = scaler.transform(X_test_encoded)

# Convert y_train to NumPy array
y_train = y_train.values

# Start MLFlow Experiment
mlflow.set_experiment("New_custom_Experiment_TimeBasedFeatures")

with mlflow.start_run(run_name="Outlier_Detection_Handling"):
    # Model Training and Evaluation Without Handling Outliers
    rf_without_outliers = RandomForestClassifier(random_state=42, n_estimators=100)
    rf_without_outliers.fit(X_train_scaled, y_train)
    y_pred_without_outliers = rf_without_outliers.predict(X_test_scaled)

    accuracy_without_outliers = accuracy_score(y_test, y_pred_without_outliers)
    f1_without_outliers = f1_score(y_test, y_pred_without_outliers, average="weighted")

    print("Performance Without Handling Outliers:")
    print(f"Accuracy: {accuracy_without_outliers}")
    print(f"F1-Score: {f1_without_outliers}")
    print(classification_report(y_test, y_pred_without_outliers))

    # Detect and Handle Outliers in Training Data Using Z-Score
    def cap_outliers(data, threshold=3):
        z_scores = np.abs((data - np.mean(data, axis=0)) / np.std(data, axis=0))
        return np.clip(data, -threshold, threshold)

    X_train_capped = cap_outliers(X_train_scaled)
    X_test_capped = cap_outliers(X_test_scaled)

    # Model Training and Evaluation After Handling Outliers
    rf_with_outliers = RandomForestClassifier(random_state=42, n_estimators=100)
    rf_with_outliers.fit(X_train_capped, y_train)
    y_pred_with_outliers = rf_with_outliers.predict(X_test_capped)

    accuracy_with_outliers = accuracy_score(y_test, y_pred_with_outliers)
    f1_with_outliers = f1_score(y_test, y_pred_with_outliers, average="weighted")

    print("\nPerformance After Handling Outliers:")
    print(f"Accuracy: {accuracy_with_outliers}")
    print(f"F1-Score: {f1_with_outliers}")
    print(classification_report(y_test, y_pred_with_outliers))

    # Log Results in MLFlow
    mlflow.log_param("method", "Outlier Detection and Handling")
    mlflow.log_metric("accuracy_without_outliers", accuracy_without_outliers)
    mlflow.log_metric("f1_score_without_outliers", f1_without_outliers)
    mlflow.log_metric("accuracy_with_outliers", accuracy_with_outliers)
    mlflow.log_metric("f1_score_with_outliers", f1_with_outliers)
    mlflow.sklearn.log_model(rf_with_outliers, artifact_path="RandomForest_OutlierHandled_Model")

    print("Custom Outlier Handling Experiment Complete. Results logged in MLFlow.")

#F1-score plots to compare experiments and determine the best model.

import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd
import matplotlib.pyplot as plt

# Initialize MLFlow client
client = MlflowClient()

# Specify the experiment name
experiment_name = "New_custom_Experiment_TimeBasedFeatures"  # Replace with the correct experiment name
experiment = client.get_experiment_by_name(experiment_name)

# Fetch all runs in the experiment
runs = client.search_runs(experiment_ids=[experiment.experiment_id])

# Extract F1-scores and relevant parameters
data = []
for run in runs:
    run_id = run.info.run_id
    run_name = run.data.tags.get("mlflow.runName", "Unnamed Run")
    f1_without_outliers = run.data.metrics.get("f1_score_without_outliers")
    f1_with_outliers = run.data.metrics.get("f1_score_with_outliers")
    data.append({
        "Run Name": run_name,
        "Run ID": run_id,
        "F1-Score (Without Outliers)": f1_without_outliers,
        "F1-Score (With Outliers)": f1_with_outliers
    })

# Create a DataFrame
df = pd.DataFrame(data)

# Melt DataFrame for easier plotting
df_melted = df.melt(id_vars=["Run Name", "Run ID"],
                    value_vars=["F1-Score (Without Outliers)", "F1-Score (With Outliers)"],
                    var_name="Metric",
                    value_name="F1-Score")

# Plot F1-scores
plt.figure(figsize=(10, 6))
for key, grp in df_melted.groupby("Metric"):
    plt.bar(grp["Run Name"], grp["F1-Score"], label=key)

plt.xlabel("Run Name")
plt.ylabel("F1-Score")
plt.title("F1-Score Comparison Across Runs")
plt.xticks(rotation=45)
plt.legend()
plt.tight_layout()

# Show plot
plt.show()

# Print the best model
best_row = df_melted.loc[df_melted["F1-Score"].idxmax()]
print(f"Best Model: {best_row['Run Name']} ({best_row['Metric']}), F1-Score: {best_row['F1-Score']:.3f}")

import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd
import matplotlib.pyplot as plt

# Initialize MLFlow client
client = MlflowClient()

# List of experiment names
experiment_names = [
    "Feature_Selection_LungCancer",
    "PCA_Dimensionality_Reduction",
    "Custom_Experiment_RFE_RF",
    "Custom_Experiment_InteractionFeatures",
    "New_custom_Experiment_TimeBasedFeatures"
]

# Dictionary to store experiment data
data = []

# Iterate over each experiment to fetch metrics
for exp_name in experiment_names:
    experiment = client.get_experiment_by_name(exp_name)
    if experiment is not None:
        runs = client.search_runs(experiment_ids=[experiment.experiment_id])
        for run in runs:
            run_id = run.info.run_id
            run_name = run.data.tags.get("mlflow.runName", "Unnamed Run")
            f1_score = run.data.metrics.get("f1_score", None)
            if f1_score is not None:
                data.append({
                    "Experiment": exp_name,
                    "Run Name": run_name,
                    "Run ID": run_id,
                    "F1-Score": f1_score
                })

# Create a DataFrame from the collected data
df = pd.DataFrame(data)

# Check if there is data to plot
if not df.empty:
    # Plot F1-scores for each experiment
    plt.figure(figsize=(12, 8))
    for exp_name, grp in df.groupby("Experiment"):
        plt.bar(grp["Run Name"], grp["F1-Score"], label=exp_name)

    # Add labels and legend
    plt.xlabel("Run Name")
    plt.ylabel("F1-Score")
    plt.title("F1-Score Comparison Across Experiments")
    plt.xticks(rotation=45, ha="right")
    plt.legend(title="Experiment")
    plt.tight_layout()

    # Display the plot
    plt.show()

    # Highlight the best model
    best_row = df.loc[df["F1-Score"].idxmax()]
    print(f"Best Model: {best_row['Run Name']} in Experiment {best_row['Experiment']} with F1-Score: {best_row['F1-Score']:.3f}")
else:
    print("No F1-scores found for the specified experiments.")

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
from sklearn.preprocessing import LabelEncoder

# Load your dataset
df = pd.read_csv("lung_cancer.csv")

# Inspect data types
print(df.dtypes)

# Convert categorical columns to numeric
label_encoders = {}
categorical_columns = ["GENDER"]  # Add other categorical columns if needed

for column in categorical_columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

# Split dataset into features (X) and target (y)
X = df.drop(columns=["LUNG_CANCER"])  # Replace 'LUNG_CANCER' with your target column name
y = df["LUNG_CANCER"]

# Ensure target variable is numeric (if not, encode it)
if y.dtype == "object":
    y = LabelEncoder().fit_transform(y)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForest model
best_model = RandomForestClassifier(random_state=42)
best_model.fit(X_train, y_train)

# Evaluate the model
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy}")

# Save the model
joblib.dump(best_model, "final_model.pkl")
print("Model saved as final_model.pkl")

!pip install FastAPI
!pip install pydantic
!pip install joblib
!pip install numpy
from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import numpy as np

# Load the saved model
model = joblib.load("final_model.pkl")

# Initialize the FastAPI app
app = FastAPI()

# Define the input schema
class ModelInput(BaseModel):
    feature1: float
    feature2: float
    feature3: float
    feature4: float
    # Add as many fields as your model features require

@app.get("/")
def read_root():
    return {"message": "Welcome to the Lung Cancer Prediction API"}

@app.post("/predict/")
def predict(input_data: ModelInput):
    # Prepare the input data for the model
    input_array = np.array([[input_data.feature1, input_data.feature2, input_data.feature3, input_data.feature4]])
    prediction = model.predict(input_array)
    return {"prediction": int(prediction[0])}

import joblib

# Save the trained model
joblib.dump(best_model, "final_model.pkl")

# Download the model to your local system
from google.colab import files
files.download("final_model.pkl")

import joblib

# Load the model
model = joblib.load("final_model.pkl")

# Print the features expected by the model
print("Features expected by the model:", model.feature_names_in_)

import joblib
import pandas as pd

# Load the model
model = joblib.load("final_model.pkl")

# Example input data
input_data = {
    "GENDER": 1,
    "AGE": 30,
    "SMOKING": 1,
    "YELLOW_FINGERS": 0,
    "ANXIETY": 1,
    "PEER_PRESSURE": 1,
    "CHRONIC_DISEASE": 0,
    "FATIGUE": 1,
    "ALLERGY": 0,
    "WHEEZING": 0,
    "ALCOHOL_CONSUMING": 1,
    "COUGHING": 1,
    "SHORTNESS_OF_BREATH": 0,
    "SWALLOWING_DIFFICULTY": 1,
    "CHEST_PAIN": 0
}
input_df = pd.DataFrame([input_data])

# Ensure the input matches the model's feature order
input_df = input_df[model.feature_names_in_]

# Make a prediction
prediction = model.predict(input_df)
print("Prediction:", "Positive" if prediction[0] == 1 else "Negative")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import joblib

# Load the dataset
df = pd.read_csv("lung_cancer.csv")

# Encode categorical columns (e.g., GENDER and LUNG_CANCER)
df["GENDER"] = df["GENDER"].map({"M": 1, "F": 0})
df["LUNG_CANCER"] = df["LUNG_CANCER"].map({"YES": 1, "NO": 0})

# Ensure all columns are numeric
df = df.apply(pd.to_numeric, errors='coerce')

# Drop rows with missing values (if any)
df = df.dropna()

# Split features (X) and target (y)
X = df.drop(columns=["LUNG_CANCER"])
y = df["LUNG_CANCER"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Data preprocessing complete.")
print("Training samples:", len(X_train))
print("Testing samples:", len(X_test))

# Train a Random Forest model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy}")

# Save the model to a file
joblib.dump(model, "final_model.pkl")
print("Model saved as 'final_model.pkl'")